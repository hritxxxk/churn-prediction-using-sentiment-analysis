# test_few_prompts.py\n\nimport dspy\nimport re\nfrom prompt_variations import *\n\n# Using local model for now\nlm = dspy.LM('ollama/gemma3:270m')\ndspy.configure(lm=lm)\n\ndef extract_score(response_text):\n    # Extract numeric score from response text.\n    try:\n        # If response is a list, get the first element\n        if isinstance(response_text, list):\n            response_text = response_text[0]\n        \n        # Try to extract a number\n        numbers = re.findall(r'-?\\d+\\.?\\d*', str(response_text))\n        if numbers:\n            score = float(numbers[0])\n            # Clamp to [-1, 1] range\n            score = max(-1.0, min(1.0, score))\n            return score\n    except Exception as e:\n        print(f\"Error extracting score: {e}\")\n    return 0.0\n\ndef test_prompt(prompt_template, review):\n    # Test a single prompt with a review.\n    try:\n        prompt = prompt_template.format(review=review)\n        response = lm(prompt, max_tokens=30)\n        score = extract_score(response)\n        return score\n    except Exception as e:\n        print(f\"Error with prompt: {e}\")\n        return 0.0\n\ndef evaluate_prompt(prompt_name, prompt_template, reviews, expected_scores):\n    # Evaluate a prompt across all test reviews.\n    results = []\n    errors = 0\n    \n    for i, (review, expected) in enumerate(zip(reviews, expected_scores)):\n        try:\n            predicted = test_prompt(prompt_template, review)\n            error = abs(predicted - expected)\n            results.append({\n                'review_idx': i,\n                'expected': expected,\n                'predicted': predicted,\n                'error': error\n            })\n        except Exception as e:\n            errors += 1\n            print(f\"Error testing {prompt_name} on review {i}: {e}\")\n    \n    # Calculate average error\n    avg_error = sum(r['error'] for r in results) / len(results) if results else 1.0\n    direction_correct = sum(1 for r in results if (r['predicted'] >= 0) == (r['expected'] >= 0))\n    direction_accuracy = direction_correct / len(results) if results else 0.0\n    \n    return {\n        'prompt_name': prompt_name,\n        'avg_error': avg_error,\n        'direction_accuracy': direction_accuracy,\n        'errors': errors,\n        'results': results\n    }\n\ndef main():\n    # Test a few prompt variations.\n    print(\"Testing prompt variations with local Gemma model...\")\n    print(\"=\" * 60)\n    \n    # Define a few prompts to test\n    prompts = [\n        (\"Basic Direct\", PROMPT_BASIC),\n        (\"Few-Shot\", PROMPT_FEW_SHOT),\n        (\"Constrained\", PROMPT_CONSTRAINED)\n    ]\n    \n    # Use a smaller test set to save time\n    test_reviews = TEST_REVIEWS[:3]\n    expected_scores = EXPECTED_SCORES[:3]\n    \n    evaluations = []\n    for name, template in prompts:\n        print(f\"Testing {name}...\")\n        result = evaluate_prompt(name, template, test_reviews, expected_scores)\n        evaluations.append(result)\n        print(f\"Completed: {name}\")\n    \n    # Sort by average error (lower is better)\n    evaluations.sort(key=lambda x: x['avg_error'])\n    \n    # Print results\n    print(\"\\n\" + \"=\" * 60)\n    print(\"PROMPT EVALUATION RESULTS (sorted by average error)\")\n    print(\"=\" * 60)\n    \n    for i, eval_result in enumerate(evaluations):\n        print(f\"\\n{i+1}. {eval_result['prompt_name']}\")\n        print(f\"   Average Error: {eval_result['avg_error']:.3f}\")\n        print(f\"   Direction Accuracy: {eval_result['direction_accuracy']:.1%}\")\n        print(f\"   Errors: {eval_result['errors']}\")\n        \n        # Show sample predictions\n        if eval_result['results']:\n            print(\"   Sample Predictions:\")\n            for j, res in enumerate(eval_result['results']):\n                print(f\"     Review {res['review_idx']}: Expected={res['expected']:.1f}, Predicted={res['predicted']:.1f}\")\n    \n    # Identify best prompt\n    best_prompt = evaluations[0]\n    print(f\"\\n\" + \"=\" * 60)\n    print(f\"BEST PERFORMING PROMPT: {best_prompt['prompt_name']}\")\n    print(f\"Average Error: {best_prompt['avg_error']:.3f}\")\n    print(f\"Direction Accuracy: {best_prompt['direction_accuracy']:.1%}\")\n    print(\"=\" * 60)\n    \n    # Save the best prompt template\n    best_prompt_template = None\n    for name, template in prompts:\n        if name == best_prompt['prompt_name']:\n            best_prompt_template = template\n            break\n    \n    if best_prompt_template:\n        print(f\"\\nBest prompt template:\")\n        print(best_prompt_template)\n\nif __name__ == \"__main__\":\n    main()
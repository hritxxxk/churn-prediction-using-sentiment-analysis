# test_all_prompts.py\n\nimport dspy\nimport re\nimport concurrent.futures\nfrom prompt_variations import *\n\n# For now, let's test with the local Gemma model since we don't have the API key set up\n# Later, you can switch to Gemini by uncommenting the following lines:\n# import google.generativeai as genai\n# genai.configure(api_key=\"YOUR_GOOGLE_API_KEY\")\n# lm = dspy.LM('google/gemini-pro')\n\n# Using local model for now\nlm = dspy.LM('ollama/gemma3:270m')\ndspy.configure(lm=lm)\n\ndef extract_score(response_text):\n    \"\"\"Extract numeric score from response text.\"\"\"\n    try:\n        # If response is a list, get the first element\n        if isinstance(response_text, list):\n            response_text = response_text[0]\n        \n        # Try to extract a number\n        numbers = re.findall(r'-?\\d+\\.?\\d*', str(response_text))\n        if numbers:\n            score = float(numbers[0])\n            # Clamp to [-1, 1] range\n            score = max(-1.0, min(1.0, score))\n            return score\n    except Exception as e:\n        print(f\"Error extracting score: {e}\")\n    return 0.0\n\ndef test_prompt(prompt_template, review):\n    \"\"\"Test a single prompt with a review.\"\"\"\n    try:\n        prompt = prompt_template.format(review=review)\n        response = lm(prompt, max_tokens=30)\n        score = extract_score(response)\n        return score\n    except Exception as e:\n        print(f\"Error with prompt: {e}\")\n        return 0.0\n\ndef evaluate_prompt(prompt_name, prompt_template, reviews, expected_scores):\n    \"\"\"Evaluate a prompt across all test reviews.\"\"\"\n    results = []\n    errors = 0\n    \n    for i, (review, expected) in enumerate(zip(reviews, expected_scores)):\n        try:\n            predicted = test_prompt(prompt_template, review)\n            error = abs(predicted - expected)\n            results.append({\n                'review_idx': i,\n                'review': review[:50] + '...' if len(review) > 50 else review,\n                'expected': expected,\n                'predicted': predicted,\n                'error': error\n            })\n        except Exception as e:\n            errors += 1\n            print(f\"Error testing {prompt_name} on review {i}: {e}\")\n    \n    # Calculate average error\n    avg_error = sum(r['error'] for r in results) / len(results) if results else 1.0\n    direction_correct = sum(1 for r in results if (r['predicted'] >= 0) == (r['expected'] >= 0))\n    direction_accuracy = direction_correct / len(results) if results else 0.0\n    \n    return {\n        'prompt_name': prompt_name,\n        'avg_error': avg_error,\n        'direction_accuracy': direction_accuracy,\n        'errors': errors,\n        'results': results\n    }\n\ndef main():\n    \"\"\"Test all prompt variations.\"\"\"\n    print(\"Testing all prompt variations with local Gemma model...\")\n    print(\"=\" * 60)\n    \n    # Define prompts to test\n    prompts = [\n        (\"Basic Direct\", PROMPT_BASIC),\n        (\"Few-Shot\", PROMPT_FEW_SHOT),\n        (\"Chain-of-Thought\", PROMPT_COT),\n        (\"Constrained\", PROMPT_CONSTRAINED),\n        (\"Role-Based\", PROMPT_ROLE),\n        (\"Structured\", PROMPT_STRUCTURED)\n    ]\n    \n    # Run evaluations in parallel\n    evaluations = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n        future_to_prompt = {\n            executor.submit(evaluate_prompt, name, template, TEST_REVIEWS, EXPECTED_SCORES): name \n            for name, template in prompts\n        }\n        \n        for future in concurrent.futures.as_completed(future_to_prompt):\n            try:\n                result = future.result()\n                evaluations.append(result)\n                print(f\"Completed: {result['prompt_name']}\")\n            except Exception as e:\n                print(f\"Error evaluating prompt: {e}\")\n    \n    # Sort by average error (lower is better)\n    evaluations.sort(key=lambda x: x['avg_error'])\n    \n    # Print results\n    print(\"\\n\" + \"=\" * 60)\n    print(\"PROMPT EVALUATION RESULTS (sorted by average error)\")\n    print(\"=\" * 60)\n    \n    for i, eval_result in enumerate(evaluations):\n        print(f\"\\n{i+1}. {eval_result['prompt_name']}\")\n        print(f\"   Average Error: {eval_result['avg_error']:.3f}\")\n        print(f\"   Direction Accuracy: {eval_result['direction_accuracy']:.1%}\")\n        print(f\"   Errors: {eval_result['errors']}\")\n        \n        # Show sample predictions\n        if eval_result['results']:\n            print(\"   Sample Predictions:\")\n            for j, res in enumerate(eval_result['results'][:3]):  # Show first 3\n                print(f\"     Review {res['review_idx']}: Expected={res['expected']:.1f}, Predicted={res['predicted']:.1f}\")\n    \n    # Identify best prompt\n    best_prompt = evaluations[0]\n    print(f\"\\n\" + \"=\" * 60)\n    print(f\"BEST PERFORMING PROMPT: {best_prompt['prompt_name']}\")\n    print(f\"Average Error: {best_prompt['avg_error']:.3f}\")\n    print(f\"Direction Accuracy: {best_prompt['direction_accuracy']:.1%}\")\n    print(\"=\" * 60)\n\nif __name__ == \"__main__\":\n    main()